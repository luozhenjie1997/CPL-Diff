{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/a1356913256/python/molecular_generation/CPL-Diff/scripts'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import configparser\n",
    "import concurrent.futures\n",
    "from Bio import SeqIO, Align\n",
    "from tqdm import tqdm\n",
    "from CPLDiff.models.Denoiser import Denoiser\n",
    "from CPLDiff.utils.CPLDiffDataset import XDataset\n",
    "from CPLDiff.utils.utils import set_seed\n",
    "from CPLDiff.utils.utils import extract\n",
    "from statistics import mean, stdev\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "from collections import defaultdict, Counter\n",
    "from torch import device\n",
    "from torch.cuda import is_available\n",
    "from modlamp.descriptors import GlobalDescriptor\n",
    "from esm import FastaBatchedDataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "\"\"\"修改此项来决定要采样哪种多肽，以及引导强度\"\"\"\n",
    "sample_type = 'amp'\n",
    "assert sample_type.lower() in ['amp', 'afp', 'avp']\n",
    "cfs = 1.5\n",
    "n = 1\n",
    "sample_num = 1000"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取可以使用的硬件资源\n",
    "device = device(\"cuda:0\" if is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\"\"\"读取配置文件\"\"\"\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read('../config.ini')\n",
    "conf_dict = dict(conf.items('CPLDiff_conf'))\n",
    "set_seed(int(conf_dict['seed']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "labels = {'antimicrobial': 0, 'antifungal': 1, 'antiviral': 2}\n",
    "peptide_file = ''\n",
    "\n",
    "class LengthSampler:\n",
    "    \"\"\"\n",
    "    根据数据集长度来构建多项式分布，用于采样生成的长度，来模拟随机性\n",
    "    \"\"\"\n",
    "    def __init__(self, path, max_len=254):\n",
    "        def load_fasta_file(file_path):\n",
    "            sequences = []\n",
    "            with open(file_path, \"r\") as fasta_file:\n",
    "                for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                    sequences.append(str(record.seq))\n",
    "            return sequences\n",
    "\n",
    "        data = load_fasta_file(path)\n",
    "        self.dataset_len = np.clip([len(t) for t in data], a_min=0, a_max=max_len)\n",
    "        freqs = Counter(self.dataset_len)\n",
    "        self.distrib = []\n",
    "        for i in range(max_len + 1):\n",
    "            self.distrib.append(freqs.get(i, 0))\n",
    "\n",
    "        self.distrib = np.array(self.distrib) / np.sum(self.distrib)\n",
    "\n",
    "    def sample(self, num_samples):\n",
    "        s = np.argmax(np.random.multinomial(1, self.distrib, size=(num_samples)), axis=1)\n",
    "        return s\n",
    "\n",
    "\n",
    "def ddpm_sample(denoiser, x_t_shape, timesteps, betas, sqrt_alphas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod_prev,\n",
    "                len_list=None, use_attention=True, cfs=1., peptide_type='antimicrobial', return_attn_list=False, index=1):\n",
    "    # 用于无条件扩散去噪的条件\n",
    "    unconditional = torch.zeros(x_t_shape[0], device=device, dtype=torch.int) + 3\n",
    "    # 指定生成的多肽的条件\n",
    "    conditional = torch.zeros(x_t_shape[0], device=device, dtype=torch.int) + labels[peptide_type]\n",
    "    attn_list = None\n",
    "    if len_list is None:\n",
    "        length_sampler = LengthSampler(path='../data/train/%s.fasta' % peptide_file, max_len=int(conf_dict['max_length']))\n",
    "        # 随机生成attention mask，加2是因为要在头尾添加特殊标记\n",
    "        ones_counts = length_sampler.sample(x_t_shape[0]) + 2\n",
    "    else:\n",
    "        ones_counts = np.array(len_list) + 2\n",
    "    attention_mask = np.zeros((len(ones_counts), int(conf_dict['max_length']) + 2), dtype=int)\n",
    "    for i, count in enumerate(ones_counts):\n",
    "        attention_mask[i, :count] = 1\n",
    "    attention_mask = torch.from_numpy(attention_mask).to(device)\n",
    "    x_t_shape[0] = len(ones_counts)\n",
    "    x_t = torch.randn(x_t_shape).to(device)\n",
    "    ones = torch.ones_like(x_t)\n",
    "    for t in tqdm(reversed(range(1, timesteps + 1)), desc='index %s sampling loop time step' % index, total=timesteps):\n",
    "        batched_times = torch.full((x_t.shape[0],), t, device=device, dtype=torch.long)\n",
    "        batched_times_pre = batched_times - 1\n",
    "        alpha_cumprod_prev_t = extract(alphas_cumprod_prev, batched_times_pre, x_t_shape)\n",
    "        sqrt_alpha_cumprod_prev_t = extract(sqrt_alphas_cumprod_prev, batched_times_pre, x_t_shape)\n",
    "        beta_t = extract(betas, batched_times_pre, x_t_shape)\n",
    "        sqrt_alpha_t = extract(sqrt_alphas, batched_times_pre, x_t_shape)\n",
    "        alpha_cumprod_t = extract(alphas_cumprod, batched_times_pre, x_t_shape)\n",
    "        if use_attention:\n",
    "            if return_attn_list:\n",
    "                conditional_pred_x0, attn_list = denoiser(x_t, batched_times, y=conditional, attention_mask=attention_mask, return_attn_matrix=True)\n",
    "            else:\n",
    "                conditional_pred_x0 = denoiser(x_t, batched_times, y=conditional, attention_mask=attention_mask)\n",
    "            unconditional_pred_x0 = denoiser(x_t, batched_times, y=unconditional, attention_mask=attention_mask)\n",
    "        else:\n",
    "            if return_attn_list:\n",
    "                conditional_pred_x0, attn_list = denoiser(x_t, batched_times, y=conditional, return_attn_matrix=True)\n",
    "            else:\n",
    "                conditional_pred_x0 = denoiser(x_t, batched_times, y=conditional)\n",
    "            unconditional_pred_x0 = denoiser(x_t, batched_times, y=unconditional)\n",
    "        pred_x0 = (1 + cfs) * conditional_pred_x0 - cfs * unconditional_pred_x0\n",
    "        # 最后一个时间步无需计算均值和方差后进行重参数化\n",
    "        if t == 1:\n",
    "            x_t = pred_x0\n",
    "            continue\n",
    "        noise = torch.randn_like(x_t)\n",
    "        # 已知x0下的后验的均值\n",
    "        miu = sqrt_alpha_cumprod_prev_t * beta_t / torch.add(ones, -alpha_cumprod_t) * pred_x0 + \\\n",
    "              sqrt_alpha_t * torch.add(ones, -alpha_cumprod_prev_t) / torch.add(ones, -alpha_cumprod_t) * x_t\n",
    "        # 后验的方差，使用上限方差公式\n",
    "        sigma = torch.add(ones, -alpha_cumprod_prev_t) / torch.add(ones, -alpha_cumprod_t) * beta_t\n",
    "        # 重参数化技巧\n",
    "        x_t = miu + (0.5 * torch.log(sigma)).exp() * noise\n",
    "    return (x_t, attn_list) if return_attn_list else x_t"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at ../ESM2-8M and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "EsmLMHead(\n  (dense): Linear(in_features=320, out_features=320, bias=True)\n  (layer_norm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n  (decoder): Linear(in_features=320, out_features=33, bias=False)\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载训练好的去噪模型\n",
    "denoiser_mlp = [int(i) for i in conf_dict['denoiser_mlp'].split(',')]\n",
    "denoiser = Denoiser('../' + conf_dict['denoiser_esm_model_name'], int(conf_dict['denoiser_embedding']), denoiser_mlp).cuda()\n",
    "denoiser.load_state_dict(torch.load(\"../save_model/denoise_model.pkl\"))\n",
    "tokenizer = AutoTokenizer.from_pretrained('../' + conf_dict['denoiser_esm_model_name'], trust_remote_code=True)\n",
    "esm2_model = AutoModelForMaskedLM.from_pretrained('../' + conf_dict['denoiser_esm_model_name'], trust_remote_code=True).cuda()\n",
    "decoder = esm2_model.lm_head\n",
    "denoiser.eval()\n",
    "esm2_model.eval()\n",
    "decoder.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "global peptide_file\n",
    "if sample_type == 'amp':\n",
    "    peptide_file = 'antimicrobial'\n",
    "    train_dataset = XDataset(pd.read_csv('../data/train/antimicrobial.csv'))\n",
    "    real_file = '../data/train/antimicrobial.fasta'\n",
    "    decoder.load_state_dict(torch.load(\"../save_model/antimicrobial_decoder_model_1.pkl\"))\n",
    "elif sample_type == 'afp':\n",
    "    peptide_file = 'antifungal'\n",
    "    train_dataset = XDataset(pd.read_csv('../data/train/antifungal.csv'))\n",
    "    real_file = '../data/train/antifungal.fasta'\n",
    "    decoder.load_state_dict(torch.load(\"../save_model/antifungal_decoder_model_1.pkl\"))\n",
    "else:\n",
    "    peptide_file = 'antiviral'\n",
    "    train_dataset = XDataset(pd.read_csv('../data/train/antiviral.csv'))\n",
    "    real_file = '../data/train/antiviral.fasta'\n",
    "    decoder.load_state_dict(torch.load(\"../save_model/antiviral_decoder_model_1.pkl\"))\n",
    "if not os.path.exists('./sample_%s' % peptide_file):\n",
    "    os.mkdir('./sample_%s' % peptide_file)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "t = torch.linspace(1, int(conf_dict['time_steps']), int(conf_dict['time_steps']), device=device, dtype=torch.int)\n",
    "alphas_cumprod = 1 - torch.sqrt(t / (int(conf_dict['time_steps']) + float(conf_dict['sqrt_s'])))\n",
    "\"\"\"\n",
    "求出β列表\n",
    "\"\"\"\n",
    "betas = []\n",
    "# 记录已知的ā_t的乘项\n",
    "one_minus_beta = 0\n",
    "for i in range(int(conf_dict['time_steps'])):\n",
    "    alphas_cumprod_t = alphas_cumprod[i]\n",
    "    if i == 0:\n",
    "        # 第一项可以直接求出来\n",
    "        betas.append(1 - alphas_cumprod_t)\n",
    "        one_minus_beta = alphas_cumprod_t\n",
    "    else:\n",
    "        # 1-β_t\n",
    "        temp = alphas_cumprod_t / one_minus_beta\n",
    "        betas.append(1 - temp)\n",
    "        one_minus_beta = alphas_cumprod_t\n",
    "betas = torch.stack(betas).to(device)\n",
    "alphas = 1. - betas\n",
    "sqrt_alphas = torch.sqrt(alphas)\n",
    "# t - 1\n",
    "alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=0.0)\n",
    "sqrt_alphas_cumprod_prev = torch.sqrt(alphas_cumprod_prev)\n",
    "\n",
    "# 存储采样后的序列\n",
    "seq_list = []\n",
    "# 伪困惑度（Pseudo-Perplexity）列表\n",
    "pseudo_perplexity_list = []\n",
    "# 熵列表\n",
    "entropy_list = []\n",
    "# token键值对\n",
    "vocab_dict = {v: k for k, v in tokenizer.get_vocab().items()}\n",
    "seq_index = 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index 1 sampling loop time step: 100%|██████████████████████████████████████████████████████████████████████| 2000/2000 [04:01<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效样本数量为1000条\n",
      "去重后的数量为999条\n",
      "继续采样1条\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "index 1 sampling loop time step: 100%|██████████████████████████████████████████████████████████████████████| 2000/2000 [00:26<00:00, 76.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有效样本数量为1条\n",
      "Pseudo-Perplexity:10.580612182617188±4.615673065185547\n",
      "entropy:2.400797128677368±0.7949443459510803\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for i in range(n):\n",
    "        x0 = ddpm_sample(denoiser, [sample_num, int(conf_dict['max_length']) + 2, int(conf_dict['denoiser_embedding'])], int(conf_dict['time_steps']), betas,\n",
    "                         sqrt_alphas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod_prev, cfs=cfs, use_attention=True, peptide_type=peptide_file,\n",
    "                         index=(i + 1))\n",
    "        pred_score = decoder(x0)\n",
    "\n",
    "        x0 = x0.cpu().numpy()\n",
    "        np.save('./sample_%s/sample_x0_%s' % (peptide_file, i), x0)\n",
    "        seq_ids_list = pred_score.argmax(dim=-1)\n",
    "        for index, seq_ids in zip(range(sample_num), seq_ids_list):\n",
    "            # 跳过开头不是cls的样本\n",
    "            if seq_ids[0] == 0:\n",
    "                \"\"\"\n",
    "                跳过含有除了pad的特殊标记的，以及没有eos标记的序列\n",
    "                \"\"\"\n",
    "                if not torch.any(torch.eq(seq_ids, 3)).item() and not torch.any(torch.eq(seq_ids, 29)).item() and \\\n",
    "                    not torch.any(torch.eq(seq_ids, 30)).item() and not torch.any(torch.eq(seq_ids, 31)).item() and \\\n",
    "                        torch.any(torch.eq(seq_ids, 2)).item():\n",
    "                    eos_indexs = torch.nonzero(seq_ids == 2).squeeze()\n",
    "                    if eos_indexs.numel() > 1:\n",
    "                        eos_index = eos_indexs[0].item()\n",
    "                    else:\n",
    "                        eos_index = eos_indexs.item()\n",
    "                    # 获取生成的序列\n",
    "                    seq = tokenizer.decode(seq_ids[1:eos_index]).replace(\" \", \"\").replace(\"<cls>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
    "                    seq_list.append(seq)\n",
    "\n",
    "        total_sample = len(seq_list)\n",
    "        print(\"有效样本数量为%s条\" % total_sample)\n",
    "        surplus_num = sample_num * n - total_sample\n",
    "        de_duplication_sample_seq_set = set(seq_list)\n",
    "        print(\"去重后的数量为%s条\" % len(de_duplication_sample_seq_set))\n",
    "        \"\"\"\n",
    "        由于生成模型的随机性，因此可能会采样到重复的样本和不正常的样本，故以下while循环为了确保生成的样本不重复以及达到指定数量\n",
    "        \"\"\"\n",
    "        while total_sample != len(de_duplication_sample_seq_set) or surplus_num != 0:\n",
    "            total_num = total_sample - len(de_duplication_sample_seq_set) + surplus_num\n",
    "            print(\"继续采样%s条\" % total_num)\n",
    "            x0 = ddpm_sample(denoiser, [total_num, int(conf_dict['max_length']) + 2, int(conf_dict['denoiser_embedding'])], int(conf_dict['time_steps']), betas,\n",
    "                             sqrt_alphas, alphas_cumprod, alphas_cumprod_prev, sqrt_alphas_cumprod_prev, cfs=cfs, use_attention=True, peptide_type=peptide_file,\n",
    "                             index=1)\n",
    "            pred_score = decoder(x0)\n",
    "            seq_ids_list = pred_score.argmax(dim=-1)\n",
    "            # 去重后的样本列表\n",
    "            seq_list = list(de_duplication_sample_seq_set)\n",
    "            num = 0\n",
    "            for index, seq_ids in zip(range(total_num), seq_ids_list):\n",
    "                if seq_ids[0] == 0:\n",
    "                    if not torch.any(torch.eq(seq_ids, 3)).item() and not torch.any(torch.eq(seq_ids, 29)).item() and \\\n",
    "                            not torch.any(torch.eq(seq_ids, 30)).item() and not torch.any(torch.eq(seq_ids, 31)).item() and \\\n",
    "                            torch.any(torch.eq(seq_ids, 2)).item():\n",
    "                        eos_indexs = torch.nonzero(seq_ids == 2).squeeze()\n",
    "                        if eos_indexs.numel() > 1:\n",
    "                            eos_index = eos_indexs[0].item()\n",
    "                        else:\n",
    "                            eos_index = eos_indexs.item()\n",
    "                        seq = tokenizer.decode(seq_ids[1: eos_index]).replace(\" \", \"\").replace(\"<cls>\", \"\").replace(\"<eos>\", \"\").replace(\"<pad>\", \"\")\n",
    "                        seq_list.append(seq)\n",
    "                        num += 1\n",
    "            print(\"有效样本数量为%s条\" % num)\n",
    "            surplus_num = sample_num * n - len(seq_list)\n",
    "            de_duplication_sample_seq_set = set(seq_list)\n",
    "        else:\n",
    "            with open(\"./sample_%s/sample_result_metircs.fasta\" % peptide_file, 'w') as f:\n",
    "                for seq in seq_list:\n",
    "                    length = len(seq)\n",
    "                    # 将‘X’替换成随机选择的常见氨基酸\n",
    "                    if 'X' in seq:\n",
    "                        for i in range(length):\n",
    "                            if seq[i] == 'X':\n",
    "                                seq = seq.replace(seq[i], vocab_dict[random.randrange(4, 24)])\n",
    "                    \"\"\"\n",
    "                    伪困惑度\n",
    "                    \"\"\"\n",
    "                    tensor_input = tokenizer.encode(seq, return_tensors='pt')\n",
    "                    repeat_input = tensor_input.repeat(tensor_input.size(-1) - 2, 1)\n",
    "                    # 根据伪困惑度的公式，除 [CLS] 和 [SEP] 外，逐个屏蔽\n",
    "                    mask = torch.ones(tensor_input.size(-1) - 1).diag(1)[:-2]\n",
    "                    masked_input = repeat_input.masked_fill(mask == 1, tokenizer.mask_token_id)\n",
    "                    # \"-100\"表示计算交叉熵时不计算该部分\n",
    "                    labels = repeat_input.masked_fill(masked_input != tokenizer.mask_token_id, -100).to(device)\n",
    "                    # esm2的loss默认是求平均后才返回的\n",
    "                    loss = esm2_model(masked_input.to(device), labels=labels.to(device)).loss\n",
    "                    pseudo_perplexity = loss.exp()\n",
    "                    pseudo_perplexity_list.append(pseudo_perplexity)\n",
    "                    \"\"\"\n",
    "                    信息熵\n",
    "                    \"\"\"\n",
    "                    entropy_dic = defaultdict(int)\n",
    "                    for amino in seq:\n",
    "                        entropy_dic[amino] += 1\n",
    "                    entropy = 0\n",
    "                    for key in entropy_dic.keys():\n",
    "                        entropy += -(entropy_dic[key] / length) * math.log2(entropy_dic[key] / length)\n",
    "                    entropy_list.append(entropy)\n",
    "                    f.write(\">%s\\n%s\\n\" % (seq_index, seq))\n",
    "                    seq_index += 1\n",
    "\n",
    "        pseudo_perplexity_list = torch.stack(pseudo_perplexity_list).to(device)\n",
    "        entropy_list = torch.tensor(entropy_list, device=device)\n",
    "        print(\"Pseudo-Perplexity:%s±%s\" % (pseudo_perplexity_list.mean().item(), torch.sqrt(pseudo_perplexity_list.var()).item()))\n",
    "        print(\"entropy:%s±%s\" % (entropy_list.mean().item(), torch.sqrt(entropy_list.var()).item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instability: 37.9084±59.1458\n"
     ]
    }
   ],
   "source": [
    "# 不稳定性得分\n",
    "desc = GlobalDescriptor(\"./sample_%s/sample_result_metircs.fasta\" % peptide_file)\n",
    "desc.instability_index()\n",
    "instability_score = desc.descriptor.squeeze()\n",
    "print(\"Instability: %.4f±%.4f\" % (mean(instability_score), stdev(instability_score)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Peptide similarity calculation::  85%|████████████████████████████████████████████████████████████▌          | 853/1000 [03:34<00:34,  4.31it/s]"
     ]
    }
   ],
   "source": [
    "# 相似性得分\n",
    "def match_score(sample_seq, real_file):\n",
    "    \"\"\"\n",
    "    相似性得分包括评估生成的多肽序列与相应多肽数据集中现有序列之间的比对得分。比对得分越低，说明生成的多肽序列越新颖\n",
    "    \"\"\"\n",
    "    aligner = Align.PairwiseAligner()\n",
    "    aligner.substitution_matrix = Align.substitution_matrices.load(\"BLOSUM62\")\n",
    "\n",
    "    score_list = []\n",
    "\n",
    "    for record in SeqIO.parse(real_file, \"fasta\"):\n",
    "        amp_str = record.seq\n",
    "        alignments = aligner.align(amp_str, sample_seq)\n",
    "        score = alignments.score\n",
    "\n",
    "        score_list.append(score)\n",
    "\n",
    "    score_list = np.stack(score_list)\n",
    "    return sample_seq, score_list.mean()\n",
    "\n",
    "sample_dataset = FastaBatchedDataset.from_file(\"./sample_%s/sample_result_metircs.fasta\" % peptide_file)\n",
    "match_score_list = []\n",
    "sample_len = len(sample_dataset)\n",
    "for i, data in zip(tqdm(range(0, sample_len), desc='Peptide similarity calculation:', total=sample_len), sample_dataset):\n",
    "    seq, score = match_score(data[1], '../data/train/%s.fasta'% peptide_file)\n",
    "    match_score_list.append(score)\n",
    "print(\"Similarity: %.4f±%.4f\" % (mean(match_score_list), stdev(match_score_list)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}