{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "'/home/a1356913256/python/molecular_generation/CPL-Diff/scripts'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import configparser\n",
    "import pandas as pd\n",
    "import torch.nn.functional as F\n",
    "from CPLDiff.models.Denoiser import Denoiser\n",
    "from CPLDiff.models.layers.EMA import EMA\n",
    "from CPLDiff.utils.CPLDiffDataset import XYDataset\n",
    "from CPLDiff.utils.utils import set_seed\n",
    "from CPLDiff.utils.utils import extract\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from colorama import Fore, Style\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from torch import device\n",
    "from torch.cuda import is_available"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cuda', index=0)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取可以使用的硬件资源\n",
    "device = device(\"cuda:0\" if is_available() else \"cpu\")\n",
    "device"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "0.0005"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"读取配置文件\"\"\"\n",
    "conf = configparser.ConfigParser()\n",
    "conf.read('../config.ini')\n",
    "conf_dict = dict(conf.items('CPLDiff_conf'))\n",
    "float(conf_dict['denoiser_lr'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "set_seed(int(conf_dict['seed']))\n",
    "if not os.path.exists('../save_model'):\n",
    "    os.mkdir('../save_model')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "\"\"\"设置扩散模型的噪声计划表\"\"\"\n",
    "# 《Diffusion-LM Improves Controllable Text Generation》采取如下时间表形式\n",
    "t = torch.arange(1, int(conf_dict['time_steps']) + 1, dtype=torch.long, device=device)\n",
    "# 求上头带横线的α\n",
    "alphas_cumprod = 1 - torch.sqrt(t / (int(conf_dict['time_steps']) + float(conf_dict['sqrt_s'])))\n",
    "# 求上头带横线的α，然后开根号\n",
    "sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)\n",
    "# 求1减去上头带横线的α，然后开根号\n",
    "sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at ../ESM2-8M and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "EsmModel(\n  (embeddings): EsmEmbeddings(\n    (word_embeddings): Embedding(33, 320, padding_idx=1)\n    (dropout): Dropout(p=0.2, inplace=False)\n    (position_embeddings): Embedding(1026, 320, padding_idx=1)\n  )\n  (encoder): EsmEncoder(\n    (layer): ModuleList(\n      (0): EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=320, out_features=320, bias=True)\n            (key): Linear(in_features=320, out_features=320, bias=True)\n            (value): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=320, out_features=1280, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=1280, out_features=320, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n      )\n      (1): EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=320, out_features=320, bias=True)\n            (key): Linear(in_features=320, out_features=320, bias=True)\n            (value): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=320, out_features=1280, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=1280, out_features=320, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n      )\n      (2): EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=320, out_features=320, bias=True)\n            (key): Linear(in_features=320, out_features=320, bias=True)\n            (value): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=320, out_features=1280, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=1280, out_features=320, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n      )\n      (3): EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=320, out_features=320, bias=True)\n            (key): Linear(in_features=320, out_features=320, bias=True)\n            (value): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=320, out_features=1280, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=1280, out_features=320, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n      )\n      (4): EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=320, out_features=320, bias=True)\n            (key): Linear(in_features=320, out_features=320, bias=True)\n            (value): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=320, out_features=1280, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=1280, out_features=320, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n      )\n      (5): EsmLayer(\n        (attention): EsmAttention(\n          (self): EsmSelfAttention(\n            (query): Linear(in_features=320, out_features=320, bias=True)\n            (key): Linear(in_features=320, out_features=320, bias=True)\n            (value): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n            (rotary_embeddings): RotaryEmbedding()\n          )\n          (output): EsmSelfOutput(\n            (dense): Linear(in_features=320, out_features=320, bias=True)\n            (dropout): Dropout(p=0.2, inplace=False)\n          )\n          (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n        )\n        (intermediate): EsmIntermediate(\n          (dense): Linear(in_features=320, out_features=1280, bias=True)\n        )\n        (output): EsmOutput(\n          (dense): Linear(in_features=1280, out_features=320, bias=True)\n          (dropout): Dropout(p=0.2, inplace=False)\n        )\n        (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n      )\n    )\n    (emb_layer_norm_after): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n  )\n  (pooler): EsmPooler(\n    (dense): Linear(in_features=320, out_features=320, bias=True)\n    (activation): Tanh()\n  )\n  (contact_head): EsmContactPredictionHead(\n    (regression): Linear(in_features=120, out_features=1, bias=True)\n    (activation): Sigmoid()\n  )\n)"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# token编码器\n",
    "tokenizer = EsmTokenizer.from_pretrained('../' + conf_dict['denoiser_esm_model_name'])\n",
    "# 用于获取蛋白质潜空间嵌入\n",
    "esm2_model = EsmModel.from_pretrained('../' + conf_dict['denoiser_esm_model_name'], add_pooling_layer=True).to(device)\n",
    "esm2_model.eval()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# 加载数据集\n",
    "train_dataset = XYDataset(pd.read_csv('../data/train/mulit_peptide_train.csv'))\n",
    "train_data_loader = torch.utils.data.DataLoader(train_dataset, batch_size=int(conf_dict['diffusion_batch_size']), pin_memory=True, shuffle=True,\n",
    "                                                persistent_workers=True, num_workers=8)\n",
    "val_dataset = XYDataset(pd.read_csv('../data/train/mulit_peptide_val.csv'))\n",
    "val_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1024, pin_memory=True, persistent_workers=True, num_workers=8)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at ../ESM2-8M and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Denoiser(\n  (time_emb): Sequential(\n    (0): SinusoidalPositionEmbeddings()\n    (1): Linear(in_features=640, out_features=1280, bias=True)\n    (2): SiLU()\n    (3): Linear(in_features=1280, out_features=1280, bias=True)\n  )\n  (label_emb): LabelEmbedder(\n    (embedding_table): Embedding(4, 1280)\n  )\n  (esm_attention_list): ModuleList(\n    (0): EsmAttention(\n      (self): EsmSelfAttention(\n        (query): Linear(in_features=320, out_features=320, bias=True)\n        (key): Linear(in_features=320, out_features=320, bias=True)\n        (value): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (rotary_embeddings): RotaryEmbedding()\n      )\n      (output): EsmSelfOutput(\n        (dense): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (1): EsmAttention(\n      (self): EsmSelfAttention(\n        (query): Linear(in_features=320, out_features=320, bias=True)\n        (key): Linear(in_features=320, out_features=320, bias=True)\n        (value): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (rotary_embeddings): RotaryEmbedding()\n      )\n      (output): EsmSelfOutput(\n        (dense): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (2): EsmAttention(\n      (self): EsmSelfAttention(\n        (query): Linear(in_features=320, out_features=320, bias=True)\n        (key): Linear(in_features=320, out_features=320, bias=True)\n        (value): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (rotary_embeddings): RotaryEmbedding()\n      )\n      (output): EsmSelfOutput(\n        (dense): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (3): EsmAttention(\n      (self): EsmSelfAttention(\n        (query): Linear(in_features=320, out_features=320, bias=True)\n        (key): Linear(in_features=320, out_features=320, bias=True)\n        (value): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (rotary_embeddings): RotaryEmbedding()\n      )\n      (output): EsmSelfOutput(\n        (dense): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (4): EsmAttention(\n      (self): EsmSelfAttention(\n        (query): Linear(in_features=320, out_features=320, bias=True)\n        (key): Linear(in_features=320, out_features=320, bias=True)\n        (value): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (rotary_embeddings): RotaryEmbedding()\n      )\n      (output): EsmSelfOutput(\n        (dense): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n    (5): EsmAttention(\n      (self): EsmSelfAttention(\n        (query): Linear(in_features=320, out_features=320, bias=True)\n        (key): Linear(in_features=320, out_features=320, bias=True)\n        (value): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n        (rotary_embeddings): RotaryEmbedding()\n      )\n      (output): EsmSelfOutput(\n        (dense): Linear(in_features=320, out_features=320, bias=True)\n        (dropout): Dropout(p=0.2, inplace=False)\n      )\n      (LayerNorm): LayerNorm((320,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (time_emb_proj_list): ModuleList(\n    (0): Sequential(\n      (0): SiLU()\n      (1): Linear(in_features=1280, out_features=640, bias=True)\n    )\n    (1): Sequential(\n      (0): SiLU()\n      (1): Linear(in_features=1280, out_features=640, bias=True)\n    )\n    (2): Sequential(\n      (0): SiLU()\n      (1): Linear(in_features=1280, out_features=640, bias=True)\n    )\n    (3): Sequential(\n      (0): SiLU()\n      (1): Linear(in_features=1280, out_features=640, bias=True)\n    )\n    (4): Sequential(\n      (0): SiLU()\n      (1): Linear(in_features=1280, out_features=640, bias=True)\n    )\n    (5): Sequential(\n      (0): SiLU()\n      (1): Linear(in_features=1280, out_features=640, bias=True)\n    )\n  )\n  (norm_before): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n  (norm_after): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n  (mlp): Sequential(\n    (0): Linear(in_features=320, out_features=320, bias=True)\n    (1): LayerNorm((320,), eps=1e-06, elementwise_affine=True)\n    (2): GELU(approximate='none')\n    (3): Linear(in_features=320, out_features=320, bias=True)\n  )\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去噪模型\n",
    "denoiser_mlp = [int(i) for i in conf_dict['denoiser_mlp'].split(',')]\n",
    "denoiser = Denoiser('../' + conf_dict['denoiser_esm_model_name'], int(conf_dict['denoiser_embedding']), denoiser_mlp).to(device)\n",
    "optimizer = torch.optim.AdamW(denoiser.parameters(), lr=float(conf_dict['denoiser_lr']), weight_decay=float(conf_dict['denoiser_weight_decay']))\n",
    "scheduler = CosineLRScheduler(optimizer, t_initial=200_000, lr_min=float(conf_dict['min_denoiser_lr']), warmup_lr_init=1e-8, warmup_t=10_000, cycle_limit=1,\n",
    "                              t_in_epochs=False)\n",
    "# 初始化EMA\n",
    "ema = EMA(denoiser, 0.99)\n",
    "ema.register()\n",
    "denoiser"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The total number of iteration steps is \u001B[92m680\u001B[0m.\n"
     ]
    }
   ],
   "source": [
    "print(\"The total number of iteration steps is \" + Fore.LIGHTGREEN_EX + \"%s\" % int(conf_dict['denoiser_epoch']) + Style.RESET_ALL + \".\")\n",
    "epoch = 0\n",
    "x_steps = 1\n",
    "# 检查是否存在之前的检查点文件\n",
    "if os.path.exists(\"../save_model/checkpoint.pth\"):\n",
    "    # 加载模型状态\n",
    "    checkpoint = torch.load(\"../save_model/checkpoint.pth\")\n",
    "    denoiser.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    ema = checkpoint['ema']\n",
    "    # 由于保存的是已经训练好的epoch，因此从下一个epoch开始\n",
    "    epoch = checkpoint['epoch'] + 1\n",
    "    x_steps = checkpoint['x_steps']\n",
    "    print(f\"Continuing from epoch {epoch}\")\n",
    "    del checkpoint\n",
    "writer = SummaryWriter(log_dir='./logs')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:129   \u001B[91mloss:0.0154\u001B[0m:  97%|██████████████████████████████████████████████████████▍ | 286/294 [00:14<00:00, 21.92it/s]\u001B[0m"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=len(train_data_loader), miniters=0)\n",
    "end_train = False\n",
    "for i in range(epoch, int(conf_dict['denoiser_epoch'])):\n",
    "    if i % 100 == 0:\n",
    "        set_seed(int(conf_dict['seed']) + i)\n",
    "    \"\"\"\n",
    "    为避免图像重叠，每隔一定的epoch创建一个log文件;\n",
    "    如果是从非设定的阈值的倍数的epoch中断的，则删掉最新的log文件，避免重叠；\n",
    "    为了保证能被阈值整除的记录在旧文件里，因此无需加1求余\n",
    "    \"\"\"\n",
    "    if epoch % 20 == 0:\n",
    "        writer = SummaryWriter(log_dir='./logs')\n",
    "    denoiser.train()\n",
    "    for index, datas in enumerate(train_data_loader):\n",
    "        # 获取序列的索引列表，加2是因为会在头尾额外插入开始和结束标记\n",
    "        seq_encode = tokenizer(datas['sequences'], max_length=int(conf_dict['max_length']) + 2, padding='max_length', return_tensors=\"pt\")\n",
    "        seq_ids_list, train_attention_mask = seq_encode['input_ids'].to(device), seq_encode['attention_mask'].to(device)\n",
    "        # 获取蛋白质潜空间嵌入\n",
    "        with torch.no_grad():\n",
    "            x0 = esm2_model(seq_ids_list, train_attention_mask).last_hidden_state\n",
    "\n",
    "        t = torch.randint(1, int(conf_dict['time_steps']) + 1, (x0.shape[0],), device=device, dtype=torch.long)\n",
    "        # 随机采样噪声\n",
    "        noise = torch.randn_like(x0, device=device)\n",
    "        # 按照给定的时间步取对应的系数，减1是因为下标是从0开始的\n",
    "        sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t - 1, x0.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, t - 1, x0.shape)\n",
    "        # 第一项，sqrt(ā_t)*x_0\n",
    "        term1 = sqrt_alphas_cumprod_t * x0\n",
    "        # 第二项，噪声乘上sqrt(1-ā_t)\n",
    "        term2 = sqrt_one_minus_alphas_cumprod_t * noise\n",
    "        # 加噪完毕后的数据\n",
    "        x_t = torch.add(term1, term2)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # 预测x0\n",
    "        pred_x0 = denoiser(x_t, t, y=datas['labels'].to(device), attention_mask=train_attention_mask)\n",
    "        x0_loss = F.mse_loss(pred_x0, x0)\n",
    "        x0_loss.backward()\n",
    "        # 梯度裁剪\n",
    "        torch.nn.utils.clip_grad_norm_(denoiser.parameters(), max_norm=float(conf_dict['denoiser_clip_grad']))\n",
    "        optimizer.step()\n",
    "\n",
    "        if x_steps % 20 == 0:\n",
    "            pbar.set_description_str(\"epoch:%s   \" % (epoch + 1) + Fore.LIGHTRED_EX + \"loss:%.4f\" % x0_loss.item() + Style.RESET_ALL)\n",
    "            writer.add_scalar(\"train/x0_loss\", x0_loss.item(), x_steps)\n",
    "        # 指数移动平均\n",
    "        ema.update()\n",
    "\n",
    "        x_steps += 1\n",
    "        pbar.update()\n",
    "        scheduler.step_update(x_steps)\n",
    "\n",
    "    pbar.reset()\n",
    "    pbar.clear()\n",
    "\n",
    "    val_x0_loss_list = []\n",
    "    # 验证模型对于去噪的性能\n",
    "    with torch.no_grad():\n",
    "        # eval前，将影子权重应用到模型中\n",
    "        ema.apply_shadow()\n",
    "        denoiser.eval()\n",
    "        for val_datas in val_data_loader:\n",
    "            val_seq_encode = tokenizer(val_datas['sequences'], max_length=int(conf_dict['max_length']) + 2, padding='max_length', return_tensors=\"pt\")\n",
    "            val_seq_ids_list, val_attention_mask = val_seq_encode['input_ids'].to(device), val_seq_encode['attention_mask'].to(device)\n",
    "            val_x0 = esm2_model(val_seq_ids_list, val_attention_mask).last_hidden_state\n",
    "            val_t = torch.randint(0, int(conf_dict['time_steps']), (val_x0.shape[0],), device=device).long()\n",
    "\n",
    "            val_noise = torch.randn_like(val_x0, device=device)\n",
    "            val_sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, val_t, val_x0.shape)\n",
    "            val_sqrt_one_minus_alphas_cumprod_t = extract(sqrt_one_minus_alphas_cumprod, val_t, val_x0.shape)\n",
    "            val_term1 = val_sqrt_alphas_cumprod_t * val_x0\n",
    "            val_term2 = val_sqrt_one_minus_alphas_cumprod_t * val_noise\n",
    "            val_x_t = torch.add(val_term1, val_term2)\n",
    "\n",
    "            val_pred_x0 = denoiser(val_x_t, val_t, y=val_datas['labels'].to(device), attention_mask=val_attention_mask)\n",
    "            val_x0_loss_list.append(F.mse_loss(val_pred_x0, val_x0))\n",
    "\n",
    "        val_x0_loss = torch.stack(val_x0_loss_list).mean()\n",
    "        writer.add_scalar(\"val/x0_loss\", val_x0_loss.item(), (epoch + 1))\n",
    "\n",
    "        if (epoch + 1) % 20 == 0:\n",
    "            torch.save(denoiser.state_dict(), \"../save_model/denoise_model.pkl\")\n",
    "        state = {\n",
    "            'model_state_dict': denoiser.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'scheduler_state_dict': scheduler.state_dict(),\n",
    "            'ema': ema,\n",
    "            'epoch': epoch,\n",
    "            'x_steps': x_steps\n",
    "        }\n",
    "        torch.save(state, \"../save_model/checkpoint.pth\")\n",
    "        epoch += 1\n",
    "        # eval之后，恢复原来模型的参数\n",
    "        ema.restore()\n",
    "        if end_train:\n",
    "            break\n",
    "\n",
    "    # 最后保存的模型选择应用EMA后的\n",
    "    ema.apply_shadow()\n",
    "    torch.save(denoiser.state_dict(), \"../save_model/denoise_model.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}